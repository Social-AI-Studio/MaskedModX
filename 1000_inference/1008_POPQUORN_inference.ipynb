{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "_sE8U5qJsrH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For openai>=1.0.0, openai.ChatCompletion is not supported anymore. Install an earlier version. Install cohere and tiktoken to address the Error message that recently came up:\n",
        "<blockquote>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.<br>\n",
        "llmx 0.0.15a0 requires cohere, which is not installed.<br>\n",
        "llmx 0.0.15a0 requires tiktoken, which is not installed.</blockquote>"
      ],
      "metadata": {
        "id": "bbU6bGGGssmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBeYLjotxgQ"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.27.8 cohere tiktoken\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POb532mGMpDw"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "import dotenv\n",
        "import openai\n",
        "import pandas as pd\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "CXlkej-isyiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KnmdujYjs0Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create directories to store inferences"
      ],
      "metadata": {
        "id": "gGnuWa3Ps1ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/MyDrive/datasets/POPQUORN_gpt-3.5-turbo-0301',\n",
        "            exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/datasets/POPQUORN_gpt-3.5-turbo-0613',\n",
        "            exist_ok=True)"
      ],
      "metadata": {
        "id": "uRKvVGU-s3X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authentication"
      ],
      "metadata": {
        "id": "9TuBzilyt9OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to first save your OpenAI API key in a .env file in your Google Drive, in the format:\n",
        "OPENAI_API_KEY = '...'."
      ],
      "metadata": {
        "id": "6ik8zisit8aE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzNMbn2uN065"
      },
      "outputs": [],
      "source": [
        "# OpenAI authentication on Google Colab. via\n",
        "# https://stackoverflow.com/a/77166086\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/.env')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "nDLYLVWwuXTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VesFLoXN2_I"
      },
      "outputs": [],
      "source": [
        "raw_data_texts = pd.read_csv(\n",
        "    '/content/drive/MyDrive/datasets/raw_data_texts.csv')\n",
        "raw_data_texts = raw_data_texts.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set prompts"
      ],
      "metadata": {
        "id": "fgXrZbZnzMWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHp6PaOOCfw"
      },
      "outputs": [],
      "source": [
        "def system_content(persona):\n",
        "    return inspect.cleandoc(\n",
        "        f\"\"\"From now on, act as {persona}.\n",
        "        Provide an answer that {persona} would regarding the comment.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqV9aliDPeE3"
      },
      "outputs": [],
      "source": [
        "def user_content(comment):\n",
        "    return inspect.cleandoc(f\"\"\"Comment: \\\"\\\"\\\"\n",
        "    {comment}\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    Considering the above comment on Reddit, how offensive do you think it is?\n",
        "\n",
        "    Please tell us your answer using this scale from 1 (Not offensive at all)\n",
        "    to 5 (Very offensive).\n",
        "\n",
        "    Constraint: Even if you are uncertain, you must pick either “1”, “2”, “3”,\n",
        "    “4”, or “5” without using any other words.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to call Chat Completion API with (persona) or without (no-persona default) the *system message*"
      ],
      "metadata": {
        "id": "RAsZGS2QzPr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYDs7ZKjRTN1"
      },
      "outputs": [],
      "source": [
        "# Prevent rate limit errors. via\n",
        "# https://github.com/openai/openai-cookbook/blob/main/examples/\n",
        "# How_to_handle_rate_limits.ipynb\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def inference_with_backoff(user_cont, model, system_cont=None):\n",
        "    if system_cont is None:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_cont},\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    return response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to perform model inference on **POPQUORN**"
      ],
      "metadata": {
        "id": "adT5IvWGz_qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function `predict_scores` that takes a list of personas (e.g., ['a man', 'a woman', 'a non-binary person']), a `topic` (e.g., a sociodemographic attribute such as *Gender*), and a GPT model name (e.g., gpt-3.5-turbo-0301) as required input arguments. The optional argument `base` determines if responses made by the no-persona default should be included. For each persona in the list of personas, the function iterates over each text in **POPQUORN**, calling the function `inference_with_backoff` for each text. It then adds the respective lists of scores predicted by each simulated persona as new columns to the **POPQUORN** dataframe. The dataframe is saved in TSV file format, and the function returns the dataframe."
      ],
      "metadata": {
        "id": "nU8iph_m0C_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d7KQS46RoTW"
      },
      "outputs": [],
      "source": [
        "def predict_scores(persona_list, topic, model, base=None):\n",
        "    df = raw_data_texts.copy(deep=True)\n",
        "    comments = df['text'].values\n",
        "    if base == 'Yes':\n",
        "        df['predicted_scores_base'] = [\n",
        "            inference_with_backoff(user_content(cmt),\n",
        "                                   model) for cmt in comments]\n",
        "    for persona in persona_list:\n",
        "        persona += ' in the United States'\n",
        "        df[f'predicted_scores_{persona}'.replace(' ', '_')] = [\n",
        "            inference_with_backoff(user_content(cmt),\n",
        "                                   model,\n",
        "                                   system_content(persona)) for cmt in comments]\n",
        "    df.to_csv(\n",
        "        f'/content/drive/MyDrive/datasets/POPQUORN_{model}/'\n",
        "        f'POPQUORN_predictions_{topic}_{model}.tsv',\n",
        "        sep='\\t', index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dictionaries, each with a `topic` as the key and a list of personas as the value"
      ],
      "metadata": {
        "id": "mHb-JzPbzlzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUzZcL_YYW2T"
      },
      "outputs": [],
      "source": [
        "gender = {\n",
        "    'gender': ['a man',\n",
        "               'a woman',\n",
        "               'a non-binary person']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiXfSb4Q5LAA"
      },
      "outputs": [],
      "source": [
        "race_ethnicity = {\n",
        "    'race_ethnicity': ['a White person',\n",
        "                       'a Black or African American',\n",
        "                       'an American Indian or Alaska Native',\n",
        "                       'an Asian',\n",
        "                       'a Native Hawaiian or Other Pacific Islander']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDka-JQDeKeZ"
      },
      "outputs": [],
      "source": [
        "age_range = {\n",
        "    'age_range': ['a person (aged <18)',\n",
        "                  'a person (aged 18-24)',\n",
        "                  'a person (aged 25-29)',\n",
        "                  'a person (aged 30-34)',\n",
        "                  'a person (aged 35-39)',\n",
        "                  'a person (aged 40-44)',\n",
        "                  'a person (aged 45-49)',\n",
        "                  'a person (aged 50-54)',\n",
        "                  'a person (aged 54-59)',\n",
        "                  'a person (aged 60-64)',\n",
        "                  'a person (aged >65)']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd_kXNCNbQQc"
      },
      "outputs": [],
      "source": [
        "occupation = {\n",
        "    'occupation': ['an employed person',\n",
        "                   'an unemployed person',\n",
        "                   'a student',\n",
        "                   'a retiree',\n",
        "                   'a homemaker',\n",
        "                   'a self-employed person']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ9VG3B-cNIS"
      },
      "outputs": [],
      "source": [
        "education_level = {\n",
        "    'education_level': [\n",
        "        'a person (whose education level is less than a high school diploma)',\n",
        "        'a person (whose education level is a high school diploma or '\n",
        "        'equivalent)',\n",
        "        'a person (whose education level is a college degree)',\n",
        "        'a person (whose education level is a graduate degree)']}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call the `predict_labels` function"
      ],
      "metadata": {
        "id": "Xfsb6JE3zp4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running each of the following cells could take several hours. Colab Pro+ includes continuous code execution (capped at 24 hours) and background execution capabilities, enabling you to close your browser/device while your code runs. CPU is sufficient. Alternatively, you can download the notebooks and edit the code accordingly to point to the directories on your local machine you want to use."
      ],
      "metadata": {
        "id": "T00vRfzvzrJc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZaQr1AMZtnK"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(gender.values())[0],\n",
        "    list(gender.keys())[0],\n",
        "    'gpt-3.5-turbo-0301',\n",
        "    'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnRkaHptZxpZ"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(gender.values())[0],\n",
        "    list(gender.keys())[0],\n",
        "    'gpt-3.5-turbo-0613',\n",
        "    'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtIqyGquC9O"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(race_ethnicity.values())[0],\n",
        "    list(race_ethnicity.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUYo8sZf_GnU"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(race_ethnicity.values())[0],\n",
        "    list(race_ethnicity.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvaYLvB7eNzg"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(age_range.values())[0],\n",
        "    list(age_range.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJVIdHeveRxI"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(age_range.values())[0],\n",
        "    list(age_range.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1AXn9lch4wR"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(occupation.values())[0],\n",
        "    list(occupation.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRzuCjhYh8Bp"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(occupation.values())[0],\n",
        "    list(occupation.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIhupMIIh9np"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(education_level.values())[0],\n",
        "    list(education_level.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw9MkL5QiBsR"
      },
      "outputs": [],
      "source": [
        "predict_scores(\n",
        "    list(education_level.values())[0],\n",
        "    list(education_level.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}