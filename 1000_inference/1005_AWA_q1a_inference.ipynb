{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "JFcKqoLSqH9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For openai>=1.0.0, openai.ChatCompletion is not supported anymore. Install an earlier version. Install cohere and tiktoken to address the Error message that recently came up:\n",
        "<blockquote>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.<br>\n",
        "llmx 0.0.15a0 requires cohere, which is not installed.<br>\n",
        "llmx 0.0.15a0 requires tiktoken, which is not installed.</blockquote>"
      ],
      "metadata": {
        "id": "Z324ssaDqFqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBeYLjotxgQ"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.27.8 cohere tiktoken\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POb532mGMpDw"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "import dotenv\n",
        "import openai\n",
        "import pandas as pd\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "V1YZKgt4yJBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J8RMeYShyJ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create directories to store inferences"
      ],
      "metadata": {
        "id": "-8DMt3GB1mrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/MyDrive/datasets/AWA_q1a_gpt-3.5-turbo-0301',\n",
        "            exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/datasets/AWA_q1a_gpt-3.5-turbo-0613',\n",
        "            exist_ok=True)"
      ],
      "metadata": {
        "id": "kV-1y8hK3l5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authentication"
      ],
      "metadata": {
        "id": "BXC5ayXk5tUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to first save your OpenAI API key in a .env file in your Google Drive, in the format:\n",
        "OPENAI_API_KEY = '...'."
      ],
      "metadata": {
        "id": "5a_-ql7s5wct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI authentication on Google Colab. via\n",
        "# https://stackoverflow.com/a/77166086\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/.env')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "gk4bNpBb5zuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "XAHXPPYK63xO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VesFLoXN2_I"
      },
      "outputs": [],
      "source": [
        "largeScale_tweets = pd.read_csv(\n",
        "    '/content/drive/MyDrive/datasets/largeScale_tweets.csv')\n",
        "largeScale_tweets = largeScale_tweets.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set prompts"
      ],
      "metadata": {
        "id": "oo-KRtbj7VXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHp6PaOOCfw"
      },
      "outputs": [],
      "source": [
        "def system_content(persona):\n",
        "    return inspect.cleandoc(\n",
        "        f\"\"\"From now on, act as {persona}.\n",
        "        Provide an answer that {persona} would regarding the post.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqV9aliDPeE3"
      },
      "outputs": [],
      "source": [
        "def user_content(post):\n",
        "    return inspect.cleandoc(f\"\"\"Post: \\\"\\\"\\\"\n",
        "    {post}\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    How toxic/hateful/disrespectful or offensive does this post seem to you?\n",
        "\n",
        "    Please tell us your answer using this scale from 1 (Not at all) to 5 (Very\n",
        "    much so).\n",
        "\n",
        "    Constraint: Even if you are uncertain, you must pick either “1”, “2”, “3”,\n",
        "    “4”, or “5” without using any other words.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to call Chat Completion API with (persona) or without (no-persona default) the *system message*"
      ],
      "metadata": {
        "id": "JhwO8_WB7wSj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYDs7ZKjRTN1"
      },
      "outputs": [],
      "source": [
        "# Prevent rate limit errors. via\n",
        "# https://github.com/openai/openai-cookbook/blob/main/examples/\n",
        "# How_to_handle_rate_limits.ipynb\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def inference_with_backoff(user_cont, model, system_cont=None):\n",
        "    if system_cont is None:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_cont},\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    return response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to perform model inference on **AWA**"
      ],
      "metadata": {
        "id": "CGHyeUkt8Ert"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function `predict_q1a_ratings` that takes a list of personas (e.g., ['a White/Caucasian person', 'a Black/African American']), a `topic` (e.g., a sociodemographic attribute such as *Race/Ethnicity*), and a GPT model name (e.g., gpt-3.5-turbo-0301) as required input arguments. The optional argument `base` determines if ratings made by the no-persona default should be included. For each persona in the list of personas, the function iterates over each post in **AWA**, calling the function `inference_with_backoff` for each post. It then adds the respective lists of ratings predicted by each simulated persona as new columns to the **AWA** dataframe. The dataframe is saved in TSV file format, and the function returns the dataframe."
      ],
      "metadata": {
        "id": "O_gNVZOF_ygu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d7KQS46RoTW"
      },
      "outputs": [],
      "source": [
        "def predict_q1a_ratings(persona_list, topic, model, base=None):\n",
        "    df = largeScale_tweets.copy(deep=True)\n",
        "    posts = df['tweet'].values\n",
        "    if base == 'Yes':\n",
        "        df['predicted_q1a_ratings_base'] = [\n",
        "            inference_with_backoff(user_content(post),\n",
        "                                   model) for post in posts]\n",
        "    for persona in persona_list:\n",
        "        persona += ' in the United States'\n",
        "        df[f'predicted_q1a_ratings_{persona}'.replace(' ', '_')] = [\n",
        "            inference_with_backoff(user_content(post),\n",
        "                                   model,\n",
        "                                   system_content(persona)) for post in posts]\n",
        "    df.to_csv(\n",
        "        f'/content/drive/MyDrive/datasets/AWA_q1a_{model}/'\n",
        "        f'AWA_q1a_predictions_{topic}_{model}.tsv',\n",
        "        sep='\\t', index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dictionaries, each with a `topic` as the key and a list of personas as the value"
      ],
      "metadata": {
        "id": "ZIT9Nz32EwCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiXfSb4Q5LAA"
      },
      "outputs": [],
      "source": [
        "race_ethnicity = {\n",
        "    'race_ethnicity': ['an Asian/Asian American',\n",
        "                       'a Black/African American',\n",
        "                       'a Hispanic/Latinx person',\n",
        "                       'a White/Caucasian person',\n",
        "                       'a Native Hawaiian/Pacific Islander',\n",
        "                       'a Native American/First Nations person']}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender = {\n",
        "    'gender': ['a man/male',\n",
        "               'a woman/female',\n",
        "               'a non-binary person']}"
      ],
      "metadata": {
        "id": "RUzZcL_YYW2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_range = {\n",
        "    'age_range': ['a person (aged 18-24)',\n",
        "                  'a person (aged 25-29)',\n",
        "                  'a person (aged 30-34)',\n",
        "                  'a person (aged 35-39)',\n",
        "                  'a person (aged 40-44)',\n",
        "                  'a person (aged 45-49)',\n",
        "                  'a person (aged 50-54)',\n",
        "                  'a person (aged 55-59)',\n",
        "                  'a person (aged 60-64)',\n",
        "                  'a person (aged 65+)']}"
      ],
      "metadata": {
        "id": "fQKo8doVdEjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call the `predict_q1a_ratings` function"
      ],
      "metadata": {
        "id": "Q0agW-_HM0yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running each of the following cells could take several hours. Colab Pro+ includes continuous code execution (capped at 24 hours) and background execution capabilities, enabling you to close your browser/device while your code runs. CPU is sufficient. Alternatively, you can download the notebooks and edit the code accordingly to point to the directories on your local machine you want to use."
      ],
      "metadata": {
        "id": "XKSYNQzDMpLl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtIqyGquC9O"
      },
      "outputs": [],
      "source": [
        "predict_q1a_ratings(\n",
        "    list(race_ethnicity.values())[0],\n",
        "    list(race_ethnicity.keys())[0],\n",
        "    'gpt-3.5-turbo-0301',\n",
        "    'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUYo8sZf_GnU"
      },
      "outputs": [],
      "source": [
        "predict_q1a_ratings(\n",
        "    list(race_ethnicity.values())[0],\n",
        "    list(race_ethnicity.keys())[0],\n",
        "    'gpt-3.5-turbo-0613',\n",
        "    'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_q1a_ratings(\n",
        "    list(gender.values())[0],\n",
        "    list(gender.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ],
      "metadata": {
        "id": "wZaQr1AMZtnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_q1a_ratings(\n",
        "    list(gender.values())[0],\n",
        "    list(gender.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "nnRkaHptZxpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_q1a_ratings(\n",
        "    list(age_range.values())[0],\n",
        "    list(age_range.keys())[0],\n",
        "    'gpt-3.5-turbo-0301')"
      ],
      "metadata": {
        "id": "c11Cslphek-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_q1a_ratings(\n",
        "    list(age_range.values())[0],\n",
        "    list(age_range.keys())[0],\n",
        "    'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "VHok_vZpenu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}