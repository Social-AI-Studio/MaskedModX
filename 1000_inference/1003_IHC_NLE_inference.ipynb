{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "ULCr-C7x5Q8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For openai>=1.0.0, openai.ChatCompletion is not supported anymore. Install an earlier version. Install cohere and tiktoken to address the Error message that recently came up:\n",
        "<blockquote>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.<br>\n",
        "llmx 0.0.15a0 requires cohere, which is not installed.<br>\n",
        "llmx 0.0.15a0 requires tiktoken, which is not installed.</blockquote>"
      ],
      "metadata": {
        "id": "9OP8Jd4U5qgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBeYLjotxgQ"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.27.8 cohere tiktoken\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POb532mGMpDw"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "import dotenv\n",
        "import openai\n",
        "import pandas as pd\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "ww9p7Y88582k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VS6nzcW55-Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create directories to store inferences"
      ],
      "metadata": {
        "id": "iEr_3vlA6B1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/MyDrive/datasets/IHC_NLE_gpt-3.5-turbo-0301',\n",
        "            exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/datasets/IHC_NLE_gpt-3.5-turbo-0613',\n",
        "            exist_ok=True)"
      ],
      "metadata": {
        "id": "1PtA99W76CYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authentication"
      ],
      "metadata": {
        "id": "yuALTpTG6Hj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to first save your OpenAI API key in a .env file in your Google Drive, in the format:\n",
        "OPENAI_API_KEY = '...'."
      ],
      "metadata": {
        "id": "5rPxYJPK6JOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzNMbn2uN065"
      },
      "outputs": [],
      "source": [
        "# OpenAI authentication on Google Colab. via\n",
        "# https://stackoverflow.com/a/77166086\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/.env')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "u5gid1rY6Odd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VesFLoXN2_I"
      },
      "outputs": [],
      "source": [
        "implicit_hate_stg3_posts = pd.read_csv(\n",
        "    '/content/drive/MyDrive/datasets/implicit_hate_v1_stg3_posts.tsv',\n",
        "    sep='\\t')\n",
        "implicit_hate_stg3_posts = implicit_hate_stg3_posts.sample(n=1590,\n",
        "                                                           random_state=1)\n",
        "implicit_hate_stg3_posts = implicit_hate_stg3_posts.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set prompts"
      ],
      "metadata": {
        "id": "Ia6-3Tkt6370"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHp6PaOOCfw"
      },
      "outputs": [],
      "source": [
        "def system_content(persona):\n",
        "    return inspect.cleandoc(\n",
        "        f\"\"\"From now on, act as {persona}.\n",
        "        Provide an answer that {persona} would regarding the tweet.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqV9aliDPeE3"
      },
      "outputs": [],
      "source": [
        "def user_content(tweet):\n",
        "    return inspect.cleandoc(f\"\"\"\n",
        "    The following tweet has been categorized as “implicit hate speech” in a\n",
        "    prior labeling stage; a more complex and subtle attack on a person or a\n",
        "    group of people based on their protected characteristics without relying on\n",
        "    explicit keywords.\n",
        "\n",
        "    Tweet: \\\"\\\"\\\"\n",
        "    {tweet}\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    The goal of the task is to infer both the targeted group (GROUP) and what\n",
        "    the post is actually implying about that group.\n",
        "\n",
        "    Q1) Which group of people does this post belong to? (GROUP)\n",
        "    Examples of answers are: Black folks, Asian folks, Muslims, Jews,\n",
        "    Latino/Latina folks, immigrants, etc.\n",
        "\n",
        "    Q2) What aspect/stereotype/characteristic of this group is referenced or\n",
        "    implied by the post? – Use simple phrases and do not copy paste from the\n",
        "    post.\n",
        "    Use the GROUP identified in the previous question to form a simple phrase\n",
        "    and DO NOT COPY PASTE from the post. Examples of simple phrases include but\n",
        "    are not limited to: GROUP do/does ___, GROUP are ___, GROUP kill ___,\n",
        "    GROUP have ___, GROUP commit ___\n",
        "    Examples of common stereotypes include: Women are ***, Immigrants take ***,\n",
        "    Muslims kill ***, Liberals are ***\n",
        "\n",
        "    Your response must be in a strict JSON format as follows: {{\"GROUP\": \"...\",\n",
        "    \"implied statement of implicitly hateful tweet\": \"...\"}}.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to call Chat Completion API with (persona) or without (no-persona default) the *system message*"
      ],
      "metadata": {
        "id": "zBv-w5Gh68gE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYDs7ZKjRTN1"
      },
      "outputs": [],
      "source": [
        "# Prevent rate limit errors. via\n",
        "# https://github.com/openai/openai-cookbook/blob/main/examples/\n",
        "# How_to_handle_rate_limits.ipynb\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def inference_with_backoff(user_cont, model, system_cont=None):\n",
        "    if system_cont is None:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_cont},\n",
        "                {\"role\": \"user\", \"content\": user_cont}\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "    return response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to perform model inference on **IHC**"
      ],
      "metadata": {
        "id": "Q0uNQJUu6_Fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function `predict_NLE` that takes a list of personas (e.g., ['a White person', 'a Black or African American']), a `topic` (e.g., a sociodemographic attribute such as *Race/Ethnicity*), and a GPT model name (e.g., gpt-3.5-turbo-0301) as required input arguments. The optional argument `base` determines if responses made by the no-persona default should be included. For each persona in the list of personas, the function iterates over each tweet in **IHC**, calling the function `inference_with_backoff` for each tweet. It then adds the respective lists of NLEs generated by each simulated persona as new columns to the **IHC** dataframe. The dataframe is saved in TSV file format, and the function returns the dataframe."
      ],
      "metadata": {
        "id": "mBYBBf837C4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d7KQS46RoTW"
      },
      "outputs": [],
      "source": [
        "def predict_NLE(persona_list, topic, model, base=None):\n",
        "    df = implicit_hate_stg3_posts.copy(deep=True)\n",
        "    tweets = df['post'].values\n",
        "    if base == 'Yes':\n",
        "        df['predicted_NLE_base'] = [\n",
        "            inference_with_backoff(user_content(twt),\n",
        "                                   model) for twt in tweets]\n",
        "    for persona in persona_list:\n",
        "        persona += ' in the United States'\n",
        "        df[f'predicted_NLE_{persona}'.replace(' ', '_')] = [\n",
        "            inference_with_backoff(user_content(twt),\n",
        "                                   model,\n",
        "                                   system_content(persona)) for twt in tweets]\n",
        "    df.to_csv(\n",
        "        f'/content/drive/MyDrive/datasets/IHC_NLE_{model}/'\n",
        "        f'IHC_NLE_predictions_{topic}_{model}.tsv',\n",
        "        sep='\\t', index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dictionaries, each with a `topic` as the key and a list of personas as the value"
      ],
      "metadata": {
        "id": "n0ts-SpOETEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA2iGbtrELG8"
      },
      "outputs": [],
      "source": [
        "gender_genderidentity_sexuality = {\n",
        "    'gender_genderidentity_sexuality': ['a man',\n",
        "                                        'a woman',\n",
        "                                        'a non-binary person',\n",
        "                                        'a LGBTQIA+ person']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiYbc-Fq3nyC"
      },
      "outputs": [],
      "source": [
        "religion = {'religion': ['a Muslim',\n",
        "                         'an atheist',\n",
        "                         'a Buddhist',\n",
        "                         'a Christian',\n",
        "                         'a Hindu',\n",
        "                         'a Jew']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDIkikR53xhj"
      },
      "outputs": [],
      "source": [
        "race_ethnicity = {\n",
        "    'race_ethnicity': ['a White person',\n",
        "                       'a Black or African American',\n",
        "                       'an American Indian or Alaska Native',\n",
        "                       'an Asian',\n",
        "                       'a Native Hawaiian or Other Pacific Islander',\n",
        "                       'a Hispanic/Latinx person']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA-9Uh2J4JIK"
      },
      "outputs": [],
      "source": [
        "origin = {'origin': ['an immigrant',\n",
        "                     'a refugee',\n",
        "                     'an American citizen']}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call the `predict_NLE` function"
      ],
      "metadata": {
        "id": "csRAHFGdLqAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running each of the following cells could take several hours. Colab Pro+ includes continuous code execution (capped at 24 hours) and background execution capabilities, enabling you to close your browser/device while your code runs. CPU is sufficient. Alternatively, you can download the notebooks and edit the code accordingly to point to the directories on your local machine you want to use."
      ],
      "metadata": {
        "id": "w9P_h0DELpan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlQv5l2Cm6Fr"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(gender_genderidentity_sexuality.values())[0],\n",
        "            list(gender_genderidentity_sexuality.keys())[0],\n",
        "            'gpt-3.5-turbo-0301',\n",
        "            'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9N3oFkg2ejv"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(gender_genderidentity_sexuality.values())[0],\n",
        "            list(gender_genderidentity_sexuality.keys())[0],\n",
        "            'gpt-3.5-turbo-0613',\n",
        "            'Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be_nA4Y9ofMG"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(religion.values())[0],\n",
        "            list(religion.keys())[0],\n",
        "            'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdIzNW4romam"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(religion.values())[0],\n",
        "            list(religion.keys())[0],\n",
        "            'gpt-3.5-turbo-0613')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le3c1zmDo4BN"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(race_ethnicity.values())[0],\n",
        "            list(race_ethnicity.keys())[0],\n",
        "            'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RHlFvwmo652"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(race_ethnicity.values())[0],\n",
        "            list(race_ethnicity.keys())[0],\n",
        "            'gpt-3.5-turbo-0613')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8EllPbKpO_l"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(origin.values())[0],\n",
        "            list(origin.keys())[0],\n",
        "            'gpt-3.5-turbo-0301')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca58bSFbpRet"
      },
      "outputs": [],
      "source": [
        "predict_NLE(list(origin.values())[0],\n",
        "            list(origin.keys())[0],\n",
        "            'gpt-3.5-turbo-0613')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}