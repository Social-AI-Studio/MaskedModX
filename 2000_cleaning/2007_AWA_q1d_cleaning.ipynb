{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "20fJ8J891X_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_FpuOpAXgqu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount drive"
      ],
      "metadata": {
        "id": "vDaPkpBkUdNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "03IvRW_dUbFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create directory to store cleaned data"
      ],
      "metadata": {
        "id": "w3BunacvgYxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/MyDrive/datasets/cleaning', exist_ok=True)"
      ],
      "metadata": {
        "id": "R1TyC8CMge0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import datasets"
      ],
      "metadata": {
        "id": "EXHWFJW23GXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get lists of files in **/content/drive/MyDrive/datasets/AWA_q1d_gpt-3.5-turbo-0301** and **/content/drive/MyDrive/datasets/AWA_q1d_gpt-3.5-turbo-0613** directories"
      ],
      "metadata": {
        "id": "B-nJCzHAf-OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in directories. via\n",
        "# https://www.geeksforgeeks.org/python-list-files-in-a-directory/\n",
        "dir_0301 = '/content/drive/MyDrive/datasets/AWA_q1d_gpt-3.5-turbo-0301'\n",
        "file_list_0301 = os.listdir(dir_0301)\n",
        "file_list_0301 = sorted(file_list_0301)\n",
        "dir_0613 = '/content/drive/MyDrive/datasets/AWA_q1d_gpt-3.5-turbo-0613'\n",
        "file_list_0613 = os.listdir(dir_0613)\n",
        "file_list_0613 = sorted(file_list_0613)"
      ],
      "metadata": {
        "id": "ZP3sW0ri3i8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenate files in file lists into one Pandas DataFrame for each list"
      ],
      "metadata": {
        "id": "XETT3A1-gAY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and concatenate multiple files. via\n",
        "# https://stackoverflow.com/a/21232849\n",
        "AWA_q1d_0301 = pd.concat(\n",
        "    [pd.read_csv(\"\".join((dir_0301, '/', file)), sep='\\t') for file in\n",
        "     file_list_0301],\n",
        "    axis=1)\n",
        "AWA_q1d_0613 = pd.concat(\n",
        "    [pd.read_csv(\"\".join((dir_0613, '/', file)), sep='\\t') for file in\n",
        "     file_list_0613],\n",
        "    axis=1)\n",
        "# Drop duplicate columns. via\n",
        "# https://www.geeksforgeeks.org/\n",
        "# how-to-find-drop-duplicate-columns-in-a-pandas-dataframe/\n",
        "AWA_q1d_0301 = AWA_q1d_0301.T.drop_duplicates().T\n",
        "AWA_q1d_0613 = AWA_q1d_0613.T.drop_duplicates().T"
      ],
      "metadata": {
        "id": "GF9argYz4E2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function to get the distribution of responses"
      ],
      "metadata": {
        "id": "0SnH1G-Vi8OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_responses_distribution(df, model):\n",
        "    # Value counts across multiple columns. via\n",
        "    # https://stackoverflow.com/a/61565732\n",
        "    responses_distribution = df.iloc[:, 1:].stack().value_counts()\n",
        "    # Convert value_counts output to dataframe format. via\n",
        "    # https://stackoverflow.com/a/47136484\n",
        "    responses_distribution = responses_distribution.rename_axis(\n",
        "        'Responses').reset_index(name='Count')\n",
        "    responses_distribution.to_csv(\n",
        "        f'/content/drive/MyDrive/datasets/cleaning/'\n",
        "        f'AWA_q1d_responses_distribution_{model}.csv',\n",
        "        index=False)\n",
        "    # Count null values in dataframe. via\n",
        "    # https://stackoverflow.com/questions/26266362/\n",
        "    # how-do-i-count-the-nan-values-in-a-column-in-pandas-dataframe#\n",
        "    # comment74712638_26266451\n",
        "    print('Number of unusable responses is:')\n",
        "    print(df.iloc[:, 1:].isna().sum().sum())"
      ],
      "metadata": {
        "id": "ZgqMuLelU33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_responses_distribution(AWA_q1d_0301, 'gpt-3.5-turbo-0301')\n",
        "get_responses_distribution(AWA_q1d_0613, 'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "P2RF164oVB9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove unsuccessful persona priming"
      ],
      "metadata": {
        "id": "UQt5FwM7NOdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_failed(x):\n",
        "    x = str(x)\n",
        "    x = x.lower()\n",
        "    if 'as an ai language model' in x or 'as a language model' in x:\n",
        "        return None\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "cols = list(range(21))\n",
        "cols.remove(0) # tweet\n",
        "cols.remove(14) # predicted_q1d_labels_base\n",
        "AWA_q1d_0301.iloc[:, cols] = AWA_q1d_0301.iloc[:, cols].applymap(\n",
        "    lambda x: rm_failed(x))\n",
        "AWA_q1d_0613.iloc[:, cols] = AWA_q1d_0613.iloc[:, cols].applymap(\n",
        "    lambda x: rm_failed(x))"
      ],
      "metadata": {
        "id": "TnGSnqNeM-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_responses_distribution(AWA_q1d_0301, 'gpt-3.5-turbo-0301')\n",
        "get_responses_distribution(AWA_q1d_0613, 'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "Yg8YfScdS-iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean valid responses to conform to how the data was coded, e.g., `5` if `Answer: 5`"
      ],
      "metadata": {
        "id": "W-a7ReC2TQAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remap(x):\n",
        "    # Determine if string only contains one digit. via\n",
        "    # https://stackoverflow.com/a/39799113\n",
        "    # Return True/False when using Regex. via\n",
        "    # https://stackoverflow.com/a/6577018\n",
        "    if bool(re.search(\"^\\D*1\\D*$\", str(x))) or bool(\n",
        "            re.search(\"^\\D*1.0\\D*$\", str(x))):\n",
        "        return str(1)\n",
        "    elif bool(re.search(\"^\\D*2\\D*$\", str(x))) or bool(\n",
        "            re.search(\"^\\D*2.0\\D*$\", str(x))):\n",
        "        return str(2)\n",
        "    elif bool(re.search(\"^\\D*3\\D*$\", str(x))) or bool(\n",
        "            re.search(\"^\\D*3.0\\D*$\", str(x))):\n",
        "        return str(3)\n",
        "    elif bool(re.search(\"^\\D*4\\D*$\", str(x))) or bool(\n",
        "            re.search(\"^\\D*4.0\\D*$\", str(x))):\n",
        "        return str(4)\n",
        "    elif bool(re.search(\"^\\D*5\\D*$\", str(x))) or bool(\n",
        "            re.search(\"^\\D*5.0\\D*$\", str(x))):\n",
        "        return str(5)\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "AWA_q1d_0301.iloc[:, 1:] = AWA_q1d_0301.iloc[:, 1:].applymap(\n",
        "    lambda x: remap(x))\n",
        "AWA_q1d_0613.iloc[:, 1:] = AWA_q1d_0613.iloc[:, 1:].applymap(\n",
        "    lambda x: remap(x))"
      ],
      "metadata": {
        "id": "UuoeV_i83yfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_responses_distribution(AWA_q1d_0301, 'gpt-3.5-turbo-0301')\n",
        "get_responses_distribution(AWA_q1d_0613, 'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "R0zoIy0hetr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean remaining valid responses to conform to how the data was coded, e.g., `5` if `As a Native Hawaiian/Pacific Islander in the United States, I find this post extremely offensive and disrespectful. It perpetuates harmful racial stereotypes and uses derogatory language. On a scale of 1 to 5, I would rate this post as a 5, very much racist.` (not remapped in previous round due to digits other than `5` present) AND remove invalid responses"
      ],
      "metadata": {
        "id": "5OHDksWnTWxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remap_again(x):\n",
        "    if x is not None and ('a 5' in x or 'a \"5\"' in x):\n",
        "        return str(5)\n",
        "    elif x in {str(1), str(2), str(3), str(4), str(5)}:\n",
        "        return x\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "AWA_q1d_0301.iloc[:, 1:] = AWA_q1d_0301.iloc[:, 1:].applymap(\n",
        "    lambda x: remap_again(x))\n",
        "AWA_q1d_0613.iloc[:, 1:] = AWA_q1d_0613.iloc[:, 1:].applymap(\n",
        "    lambda x: remap_again(x))"
      ],
      "metadata": {
        "id": "fUJDlBTdikPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_responses_distribution(AWA_q1d_0301, 'gpt-3.5-turbo-0301')\n",
        "get_responses_distribution(AWA_q1d_0613, 'gpt-3.5-turbo-0613')"
      ],
      "metadata": {
        "id": "egbF7PXet7nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listwise deletion of cases with invalid responses"
      ],
      "metadata": {
        "id": "M9pelqDnNZbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows with null values. via\n",
        "# https://stackoverflow.com/a/14247708\n",
        "todrop = AWA_q1d_0301[AWA_q1d_0301[AWA_q1d_0301.columns[1:]].isna().any(axis=1)]['tweet'].tolist() + \\\n",
        "    AWA_q1d_0613[AWA_q1d_0613[AWA_q1d_0613.columns[1:]].isna().any(axis=1)]['tweet'].tolist()\n",
        "# Drop rows where column values match any list element. via\n",
        "# https://stackoverflow.com/a/27965417\n",
        "AWA_q1d_0301 = AWA_q1d_0301[~AWA_q1d_0301['tweet'].isin(todrop)]\n",
        "AWA_q1d_0301.reset_index(drop=True, inplace=True)\n",
        "AWA_q1d_0613 = AWA_q1d_0613[~AWA_q1d_0613['tweet'].isin(todrop)]\n",
        "AWA_q1d_0613.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "Q31MbG2RGsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save cleaned responses"
      ],
      "metadata": {
        "id": "dj7ggHHNbE_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AWA_q1d_0301.to_csv(\n",
        "    '/content/drive/MyDrive/datasets/cleaning/'\n",
        "    'AWA_q1d_predictions_cleaned_gpt-3.5-turbo-0301.tsv',\n",
        "    sep='\\t', index=False)\n",
        "AWA_q1d_0613.to_csv(\n",
        "    '/content/drive/MyDrive/datasets/cleaning/'\n",
        "    'AWA_q1d_predictions_cleaned_gpt-3.5-turbo-0613.tsv',\n",
        "    sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "FRyEON_YvKRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}